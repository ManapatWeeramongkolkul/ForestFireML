{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPDA1uR9fkCY7yxGi3Xacvg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training & Evaluation Pipeline"
      ],
      "metadata": {
        "id": "COF7WWOtCQRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Model Training Specifications](https://docs.google.com/document/d/1UiDi8nyTcfMeMNIAz3KntlVZBlYrpoMAURuDccTt-wk/edit?usp=sharing)\n",
        "\n",
        "Model Evaluation: Identifying the best parameters for each model"
      ],
      "metadata": {
        "id": "kNbJ7WYUXWks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qK_KRJw0rpA9"
      },
      "outputs": [],
      "source": [
        "# Basic Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Source\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Senior Project/Datasets/test_gee.csv\")\n",
        "# df = pd.read_csv(\"/content/drive/MyDrive/Senior Project/Datasets/test.csv\")\n",
        "\n",
        "df = df.drop('Unnamed: 0', axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "BTKpuPDqvTli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df1['FireOccurred'].value_counts())\n",
        "print(\"Column numbers: \", len(df1.columns))"
      ],
      "metadata": {
        "id": "2z3siI-MvbQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder & Scaler\n",
        "\n",
        "X = df1.drop('FireOccurred', axis=1)\n",
        "y = df1['FireOccurred']\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "for i in range(len(df1.columns)-1):\n",
        "  X.iloc[:,i] = encoder.fit_transform(X.iloc[:,i])\n",
        "\n",
        "scaler = StandardScaler().fit(X)\n",
        "test = scaler.transform(X)\n",
        "\n",
        "display(X)"
      ],
      "metadata": {
        "id": "_Y_7l5u9vfb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training, Validation, Testing Split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 80:10:10\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1, shuffle=True)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=1/9, random_state=1, shuffle=True)\n",
        "\n",
        "Original = [X_train, X_val, X_test, y_train, y_val, y_test] # For reference"
      ],
      "metadata": {
        "id": "96m0rNH7vhDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if len(X_train)==len(y_train) and len(X_test) == len(y_test) and len(X_val) == len(y_val):\n",
        "  print(\"X and y data length matching\")\n",
        "else:\n",
        "  print(\"Error in data preparation pipeline\")\n",
        "print()\n",
        "print(\"No. of training data = %d\" % len(X_train))\n",
        "print(\"No. of validation data = %d\" % len(X_val))\n",
        "print(\"No. of testing data = %d\" % len(X_test))"
      ],
      "metadata": {
        "id": "oihO76nhvifX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(y_test.value_counts())"
      ],
      "metadata": {
        "id": "1ZcyP7nRvlYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SMOTE\n",
        "\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE \n",
        "\n",
        "print('Original dataset shape %s' % Counter(y_train))\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
        "print('Resampled dataset shape %s' % Counter(y_train))"
      ],
      "metadata": {
        "id": "T8MI03z-vmdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, recall_score, f1_score, roc_auc_score, accuracy_score\n",
        "\n",
        "def evaluation_metrics(y_true, y_pred):\n",
        "  cfm = confusion_matrix(y_true, y_pred).ravel()\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  recs = recall_score(y_true, y_pred, average='binary')\n",
        "  f1s = f1_score(y_true, y_pred, average='binary')\n",
        "  rocs = roc_auc_score(y_true, y_pred, average='macro')\n",
        "  return [cfm, acc, recs, f1s, rocs]"
      ],
      "metadata": {
        "id": "5pMqZtYUvo3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion matrix format : [ tn , fp , fn , tp ]"
      ],
      "metadata": {
        "id": "bDAw6kBSvqrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store Model Parameters and Eval\n",
        "\n",
        "models = pd.DataFrame(columns = ['model_name', 'model', 'parameters'])\n",
        "models_eval = pd.DataFrame(columns = ['model_name', 'confusion_matrix', 'accuracy', 'recall', 'f1_score', 'roc_auc_score'])"
      ],
      "metadata": {
        "id": "2m4kHhLmvnxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ML Algorithms\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import xgboost\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm\n",
        "from lightgbm import LGBMClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Save Model\n",
        "\n",
        "import pickle"
      ],
      "metadata": {
        "id": "fFvGo3IYwC3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression\n",
        "\n",
        "- Library: Scikit-learn"
      ],
      "metadata": {
        "id": "ez3z3utYt1Y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "name = 'log_clf'\n",
        "\n",
        "{'warm_start': False,\n",
        " 'solver': 'newton-cg',\n",
        " 'penalty': 'none',\n",
        " 'max_iter': 247,\n",
        " 'dual': False,\n",
        " 'C': 0}\n",
        "\n",
        "\n",
        "train = pd.DataFrame(columns = ['penalty', 'warm_start', 'solver', 'max_iter', 'dual', 'n_jobs','random_state'])\n",
        "train = train.append({'penalty' : 'none', 'warm_start': False, 'solver': 'newton-cg',  'max_iter': 247,  'dual': False, 'n_jobs': -1, 'random_state': 10}, ignore_index=True)\n",
        "train = train.append({'penalty' : 'l2', 'warm_start': False, 'solver': 'newton-cg',  'max_iter': 100,  'dual': False, 'n_jobs': -1, 'random_state': 10}, ignore_index=True)\n",
        "\n",
        "\n",
        "train = train.reset_index()\n",
        "for index, row in train.iterrows():\n",
        "    model_name = name + str(index)\n",
        "    log_clf = LogisticRegression(penalty = row['penalty'], n_jobs = int(row['n_jobs']), random_state = int(row['random_state']))\n",
        "    log_clf.fit(X_train, y_train)\n",
        "   \n",
        "    y_true = y_val\n",
        "    y_pred = log_clf.predict(X_val)\n",
        "    evaluation_results = evaluation_metrics(y_true, y_pred)\n",
        "\n",
        "    models = models.append({'model_name': model_name, \n",
        "                            'model': log_clf, \n",
        "                            'parameters': log_clf.get_params()}, \n",
        "                           ignore_index=True)\n",
        "   \n",
        "    models_eval = models_eval.append({'model_name': model_name, \n",
        "                                      'confusion_matrix' : evaluation_results[0], \n",
        "                                      'accuracy': evaluation_results[1], \n",
        "                                      'recall' : evaluation_results[2], \n",
        "                                      'f1_score': evaluation_results[3], \n",
        "                                      'roc_auc_score': evaluation_results[4]}, \n",
        "                                     ignore_index=True)"
      ],
      "metadata": {
        "id": "fiXiFiYsZSB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RandomizedSearchCV\n",
        "\n",
        "random_grid = {\n",
        "                \"penalty\": ['l1', 'l2', 'elasticnet', 'none'],\n",
        "                \"dual\": [True, False],\n",
        "                \"max_iter\" : [int(x) for x in np.linspace(100, 500, num = 20)],\n",
        "                \"warm_start\" : [True, False],\n",
        "                \"solver\" : ['lbfgs', 'newton-cg', 'liblinear'],\n",
        "                \"C\" : [int(x) for x in np.linspace(0, 1, num = 50)]\n",
        "              }\n",
        "\n",
        "log_random = RandomizedSearchCV(estimator = log_clf, \n",
        "                                param_distributions = random_grid, \n",
        "                                n_iter = 50, \n",
        "                                cv = 3, \n",
        "                                verbose=2, \n",
        "                                random_state=10)\n",
        "\n",
        "log_random.fit(X_train, y_train)\n",
        "log_random.best_params_"
      ],
      "metadata": {
        "id": "09I9B42LX8mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(models_eval)"
      ],
      "metadata": {
        "id": "w5hD6F2kX9ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machine (SVM)\n",
        "\n",
        "- Library: Scikit-learn\n",
        "- Deprecated. Poor performance no matter what."
      ],
      "metadata": {
        "id": "oBBQ_iust1eb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVMs is very computationally expensive and demanding\n",
        "\n",
        "Kernelized SVMs require the computation of a distance function between each point in the dataset, which is the dominating cost of $n$ features x $ùëõ^{2}$ observations\n",
        "\n",
        "If the cache is getting thrashed then the running time blows up to $n$ features √ó $ùëõ^{3}$ observations)\n",
        "\n",
        "Solutions:\n",
        "* Scale data to $[-1,1]$\n",
        "* Use less samples"
      ],
      "metadata": {
        "id": "KwM5PeTvdHG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_SVM = Original[0]\n",
        "X_val_SVM = Original[1]\n",
        "y_train_SVM = Original[3]\n",
        "y_val_SVM = Original[4]"
      ],
      "metadata": {
        "id": "tsq0c55qZSVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(y_train_SVM.value_counts())"
      ],
      "metadata": {
        "id": "OlCn_HS1dItl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Undersampling\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "rus = RandomUnderSampler(random_state=10)\n",
        "X_train_SVM, y_train_SVM = rus.fit_resample(X_train_SVM, y_train_SVM)"
      ],
      "metadata": {
        "id": "wPwHlXKMdiO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(y_train_SVM.value_counts())"
      ],
      "metadata": {
        "id": "9by4YC8tdj47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "name = 'svc_clf'\n",
        "\n",
        "train = pd.DataFrame(columns = ['kernel', 'random_state'])\n",
        "train = train.append({'kernel' : 'rbf', 'random_state': 10}, ignore_index=True)\n",
        "train = train.append({'kernel' : 'sigmoid', 'random_state': 10}, ignore_index=True)\n",
        "train = train.append({'kernel' : 'linear', 'random_state': 10}, ignore_index=True)\n",
        "\n",
        "train = train.reset_index()\n",
        "for index, row in train.iterrows():\n",
        "    model_name = name + str(index)\n",
        "    print(\"Currently at :\" , model_name)\n",
        "    svc_clf = SVC(kernel=row[\"kernel\"], random_state = int(row[\"random_state\"]))\n",
        "    svc_clf.fit(X_train_SVM, y_train_SVM)\n",
        "    \n",
        "    y_true = y_val_SVM\n",
        "    y_pred = svc_clf.predict(X_val_SVM)\n",
        "    evaluation_results = evaluation_metrics(y_true, y_pred)\n",
        "\n",
        "    models = models.append({'model_name': model_name, \n",
        "                            'model': svc_clf, \n",
        "                            'parameters': svc_clf.get_params()}, \n",
        "                           ignore_index=True)\n",
        "    \n",
        "    models_eval = models_eval.append({'model_name': model_name, \n",
        "                                      'confusion_matrix' : evaluation_results[0], \n",
        "                                      'accuracy': evaluation_results[1], \n",
        "                                      'recall' : evaluation_results[2], \n",
        "                                      'f1_score': evaluation_results[3], \n",
        "                                      'roc_auc_score': evaluation_results[4]}, \n",
        "                                     ignore_index=True)"
      ],
      "metadata": {
        "id": "XsSKT8gmdoVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RandomizedSearchCV\n",
        "\n",
        "random_grid = {\n",
        "              \"kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "              \"C\": np.arange(2, 10, 2),\n",
        "              \"gamma\": np.arange(0.1, 1, 0.2)\n",
        "             }\n",
        "\n",
        "svc_random = RandomizedSearchCV(estimator = svc_clf, \n",
        "                                 param_distributions = random_grid, \n",
        "                                  n_iter = 50, \n",
        "                                  cv = 3, \n",
        "                                  verbose=2, \n",
        "                                  random_state=10)\n",
        "\n",
        "svc_random.fit(X_train, y_train)\n",
        "svc_random.best_params_"
      ],
      "metadata": {
        "id": "7ZlA9AUldq5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(models_eval)"
      ],
      "metadata": {
        "id": "ouPtk_gtdt0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes\n",
        "\n",
        "- Library: Scikit-learn\n",
        "- Shuffling does not affect the model building. No random_seed.\n",
        "- RandomizedSearchCV is not appropriate since there is only 1 parameter and the value is close to 0. Choose from models_eval."
      ],
      "metadata": {
        "id": "Tzx0wl2ht1R1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "name = 'bayes_clf'\n",
        "\n",
        "train = pd.DataFrame(columns = ['var_smoothing'])\n",
        "train = train.append({'var_smoothing': 1e-10}, ignore_index=True)\n",
        "train = train.append({'var_smoothing': 1e-20}, ignore_index=True)\n",
        "train = train.append({'var_smoothing': 0}, ignore_index=True)\n",
        "\n",
        "train = train.reset_index()\n",
        "for index, row in train.iterrows():\n",
        "    model_name = name + str(index)\n",
        "    bayes_clf = GaussianNB(var_smoothing = row['var_smoothing'])\n",
        "    bayes_clf.fit(X_train, y_train)\n",
        "    \n",
        "    y_true = y_val\n",
        "    y_pred = bayes_clf.predict(X_val)\n",
        "    evaluation_results = evaluation_metrics(y_true, y_pred)\n",
        "\n",
        "    models = models.append({'model_name': model_name, \n",
        "                            'model': bayes_clf, \n",
        "                            'parameters': bayes_clf.get_params()}, \n",
        "                           ignore_index=True)\n",
        "    \n",
        "    models_eval = models_eval.append({'model_name': model_name, \n",
        "                                      'confusion_matrix' : evaluation_results[0], \n",
        "                                      'accuracy': evaluation_results[1], \n",
        "                                      'recall' : evaluation_results[2], \n",
        "                                      'f1_score': evaluation_results[3], \n",
        "                                      'roc_auc_score': evaluation_results[4]}, \n",
        "                                     ignore_index=True)"
      ],
      "metadata": {
        "id": "tAmU3wsqZSwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(models_eval)"
      ],
      "metadata": {
        "id": "n7Ad5Gr5eH_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Nearest Neighbor\n",
        "\n",
        "- Library: Scikit-learn\n",
        "- Shuffling does not affect the model building. No random_seed."
      ],
      "metadata": {
        "id": "M_gEyKKnt00q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "name = 'neigh_clf'\n",
        "\n",
        "train = pd.DataFrame(columns = ['n_neighbors', 'algorithm', 'n_jobs'])\n",
        "train = train.append({'n_neighbors': 5, 'algorithm':'auto', 'n_jobs':-1}, ignore_index=True)\n",
        "train = train.append({'n_neighbors': 1, 'algorithm':'auto', 'n_jobs':-1}, ignore_index=True)\n",
        "train = train.append({'n_neighbors': 20, 'algorithm':'kd_tree', 'n_jobs':-1}, ignore_index=True)\n",
        "\n",
        "train = train.reset_index()\n",
        "for index, row in train.iterrows():\n",
        "    model_name = name + str(index)\n",
        "    neigh_clf = KNeighborsClassifier(n_neighbors=int(row['n_neighbors']), algorithm = row['algorithm'], n_jobs = int(row['n_jobs']))\n",
        "    neigh_clf.fit(X_train, y_train)\n",
        "    \n",
        "    y_true = y_val\n",
        "    y_pred = neigh_clf.predict(X_val)\n",
        "    evaluation_results = evaluation_metrics(y_true, y_pred)\n",
        "\n",
        "    models = models.append({'model_name': model_name, \n",
        "                            'model': neigh_clf, \n",
        "                            'parameters': neigh_clf.get_params()}, \n",
        "                           ignore_index=True)\n",
        "    \n",
        "    models_eval = models_eval.append({'model_name': model_name, \n",
        "                                      'confusion_matrix' : evaluation_results[0], \n",
        "                                      'accuracy': evaluation_results[1], \n",
        "                                      'recall' : evaluation_results[2], \n",
        "                                      'f1_score': evaluation_results[3], \n",
        "                                      'roc_auc_score': evaluation_results[4]}, \n",
        "                                     ignore_index=True)"
      ],
      "metadata": {
        "id": "Vw5TiOkaZTHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RandomizedSearchCV\n",
        "\n",
        "random_grid = {\n",
        "                \"n_neighbors\": [int(x) for x in np.linspace(1, 20, num = 20)],\n",
        "                \"algorithm\": ['auto', 'kd_tree','ball_tree']\n",
        "              }\n",
        "\n",
        "neigh_random = RandomizedSearchCV(estimator = neigh_clf, \n",
        "                                  param_distributions = random_grid, \n",
        "                                  n_iter = 50, \n",
        "                                  cv = 3, \n",
        "                                  verbose=2, \n",
        "                                  random_state=10)\n",
        "\n",
        "rand = RandomizedSearchCV(knn, param_grid, cv=10, scoring='accuracy', n_iter=10, random_state=5)\n",
        "\n",
        "neigh_random.fit(X_train, y_train)\n",
        "neigh_random.best_params_"
      ],
      "metadata": {
        "id": "SlJiuLzfen_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(models_eval)"
      ],
      "metadata": {
        "id": "EvI_e6r-exRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Classifier\n",
        "\n",
        "- Library: Scikit-learn"
      ],
      "metadata": {
        "id": "e5dbRikwt1ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "name = 'rnd_clf'\n",
        "\n",
        "{'n_estimators': 300,\n",
        " 'min_samples_split': 2,\n",
        " 'min_samples_leaf': 1,\n",
        " 'max_features': 'auto',\n",
        " 'max_depth': 31}\n",
        "\n",
        "train = pd.DataFrame(columns = ['n_estimators', 'min_samples_split', 'min_samples_leaf', 'max_features','max_depth', 'n_jobs', 'random_state'])\n",
        "train = train.append({'n_estimators' : 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth' : None, 'n_jobs': -1, 'random_state': 10}, ignore_index=True)\n",
        "train = train.append({'n_estimators' : 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth' : 31, 'n_jobs': -1, 'random_state': 10}, ignore_index=True)\n",
        "train = train.append({'n_estimators' : 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth' : None, 'n_jobs': -1, 'random_state': 10}, ignore_index=True)\n",
        "\n",
        "train = train.reset_index()\n",
        "for index, row in train.iterrows():\n",
        "    model_name = name + str(index)\n",
        "    rnd_clf = RandomForestClassifier(n_estimators = int(row['n_estimators']), max_depth = None, \n",
        "                                    n_jobs = int(row['n_jobs']), random_state = int(row['random_state']))\n",
        "    rnd_clf.fit(X_train, y_train)\n",
        "    \n",
        "    y_true = y_val\n",
        "    y_pred = rnd_clf.predict(X_val)\n",
        "    evaluation_results = evaluation_metrics(y_true, y_pred)\n",
        "\n",
        "    models = models.append({'model_name': model_name, \n",
        "                            'model': rnd_clf, \n",
        "                            'parameters': rnd_clf.get_params()}, \n",
        "                           ignore_index=True)\n",
        "    \n",
        "    models_eval = models_eval.append({'model_name': model_name, \n",
        "                                      'confusion_matrix' : evaluation_results[0], \n",
        "                                      'accuracy': evaluation_results[1], \n",
        "                                      'recall' : evaluation_results[2], \n",
        "                                      'f1_score': evaluation_results[3], \n",
        "                                      'roc_auc_score': evaluation_results[4]}, \n",
        "                                     ignore_index=True)"
      ],
      "metadata": {
        "id": "_n_M-g9kZTyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RandomizedSearchCV\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 300, stop = 500, num = 20)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(15, 35, num = 7)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 3, 5]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "\n",
        "# Create the random grid\n",
        "random_grid = {\n",
        "               'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "              }\n",
        "\n",
        "rnd_random = RandomizedSearchCV(estimator = rnd_clf, \n",
        "                                param_distributions = random_grid, \n",
        "                                n_iter = 50, \n",
        "                                cv = 3, \n",
        "                                verbose=2, \n",
        "                                random_state=10)\n",
        "\n",
        "rnd_random.fit(X_train, y_train)\n",
        "rnd_random.best_params_"
      ],
      "metadata": {
        "id": "WskTBZDQe43o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(models_eval)"
      ],
      "metadata": {
        "id": "jo4WWeMIe3G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree\n",
        "\n",
        "- Library: Scikit-learn"
      ],
      "metadata": {
        "id": "brtgIH7pt1jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "name = 'tree_clf'\n",
        "\n",
        "train = pd.DataFrame(columns = ['criterion', 'splitter', 'min_samples_leaf', 'max_features', 'max_depth', 'n_jobs', 'random_state'])\n",
        "train = train.append({'criterion' : 'gini', 'splitter': 'best', 'min_samples_leaf': 1, 'max_features': 9, 'max_depth': None, 'random_state': 10}, ignore_index=True)\n",
        "train = train.append({'criterion' : 'entropy', 'splitter': 'best', 'min_samples_leaf': 1, 'max_features': 9, 'max_depth': None, 'random_state': 10}, ignore_index=True)\n",
        "train = train.append({'criterion' : 'gini', 'splitter': 'random', 'min_samples_leaf': 1, 'max_features': \"auto\", 'max_depth': None, 'random_state': 10}, ignore_index=True)\n",
        "train = train.append({'criterion' : 'entropy', 'splitter': 'random', 'min_samples_leaf': 1, 'max_features': \"auto\", 'max_depth': None, 'random_state': 10}, ignore_index=True)\n",
        "\n",
        "train = train.reset_index()\n",
        "for index, row in train.iterrows():\n",
        "    model_name = name + str(index)\n",
        "    tree_clf = DecisionTreeClassifier(criterion = row['criterion'], splitter = row['splitter'], max_depth = None, random_state = row['random_state'])\n",
        "    tree_clf.fit(X_train, y_train)\n",
        "    \n",
        "    y_true = y_val\n",
        "    y_pred = tree_clf.predict(X_val)\n",
        "    evaluation_results = evaluation_metrics(y_true, y_pred)\n",
        "\n",
        "    models = models.append({'model_name': model_name, \n",
        "                            'model': tree_clf, \n",
        "                            'parameters': tree_clf.get_params()}, \n",
        "                           ignore_index=True)\n",
        "    \n",
        "    models_eval = models_eval.append({'model_name': model_name, \n",
        "                                      'confusion_matrix' : evaluation_results[0], \n",
        "                                      'accuracy': evaluation_results[1], \n",
        "                                      'recall' : evaluation_results[2], \n",
        "                                      'f1_score': evaluation_results[3], \n",
        "                                      'roc_auc_score': evaluation_results[4]}, \n",
        "                                     ignore_index=True)"
      ],
      "metadata": {
        "id": "le5Xgsd8ZUHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RandomizedSearchCV\n",
        "\n",
        "random_grid = {\n",
        "              \"max_depth\": [3, None],\n",
        "              \"max_features\": [int(x) for x in np.linspace(1, 9, num = 9)],\n",
        "              \"min_samples_leaf\": [int(x) for x in np.linspace(1, 9, num = 9)],\n",
        "              \"criterion\": [\"gini\", \"entropy\"],\n",
        "              \"splitter\": [\"random\", \"best\"]\n",
        "              }\n",
        "\n",
        "tree_random = RandomizedSearchCV(estimator = tree_clf, \n",
        "                                 param_distributions = random_grid, \n",
        "                                 n_iter = 50, \n",
        "                                 cv = 3, \n",
        "                                 verbose=2, \n",
        "                                 random_state=10)\n",
        "\n",
        "tree_random.fit(X_train, y_train)\n",
        "tree_random.best_params_"
      ],
      "metadata": {
        "id": "8lLlrOw_fGlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(models_eval)"
      ],
      "metadata": {
        "id": "aW_Ztq6rfEiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting Classifier\n",
        "\n",
        "- Library: Scikit-learn"
      ],
      "metadata": {
        "id": "-0VzXIIcuTah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "name = 'gboost_clf'\n",
        "\n",
        "train = pd.DataFrame(columns = ['n_estimators', 'learning_rate', 'max_depth', 'random_state'])\n",
        "train = train.append({'n_estimators': 100, 'learning_rate':0.1, 'max_depth':3, 'random_state':10}, ignore_index=True)\n",
        "train = train.append({'n_estimators': 500, 'learning_rate':0.5, 'max_depth':1, 'random_state':10}, ignore_index=True)\n",
        "train = train.append({'n_estimators': 50, 'learning_rate':0.01, 'max_depth':10, 'random_state':10}, ignore_index=True)\n",
        "\n",
        "train = train.reset_index()\n",
        "for index, row in train.iterrows():\n",
        "    model_name = name + str(index)\n",
        "    gboost_clf = GradientBoostingClassifier(n_estimators=int(row['n_estimators']), learning_rate=row['learning_rate'],\n",
        "                                            max_depth=int(row['max_depth']), random_state=int(row['random_state']))\n",
        "    gboost_clf.fit(X_train, y_train)\n",
        "   \n",
        "    y_true = y_val\n",
        "    y_pred = gboost_clf.predict(X_val)\n",
        "    evaluation_results = evaluation_metrics(y_true, y_pred)\n",
        "\n",
        "    models = models.append({'model_name': model_name, \n",
        "                            'model': gboost_clf, \n",
        "                            'parameters': gboost_clf.get_params()}, \n",
        "                           ignore_index=True)\n",
        "   \n",
        "    models_eval = models_eval.append({'model_name': model_name, \n",
        "                                      'confusion_matrix' : evaluation_results[0], \n",
        "                                      'accuracy': evaluation_results[1], \n",
        "                                      'recall' : evaluation_results[2], \n",
        "                                      'f1_score': evaluation_results[3], \n",
        "                                      'roc_auc_score': evaluation_results[4]}, \n",
        "                                     ignore_index=True)"
      ],
      "metadata": {
        "id": "xRoZe1r0ZUd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost\n",
        "\n",
        "- Library: xgboost"
      ],
      "metadata": {
        "id": "J3M3ob9buTHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "name = 'xgboost_clf'\n",
        "\n",
        "train = pd.DataFrame(columns = ['n_estimators', 'learning_rate', 'max_depth', 'random_state'])\n",
        "train = train.append({'n_estimators': 1000, 'learning_rate':0.0001, 'max_depth':3, 'random_state':10}, ignore_index=True)\n",
        "train = train.append({'n_estimators': 500, 'learning_rate':0.5, 'max_depth':1, 'random_state':10}, ignore_index=True)\n",
        "train = train.append({'n_estimators': 750, 'learning_rate':0.01, 'max_depth':10, 'random_state':10}, ignore_index=True)\n",
        "\n",
        "train = train.reset_index()\n",
        "for index, row in train.iterrows():\n",
        "    model_name = name + str(index)\n",
        "    xgboost_clf = XGBClassifier(n_estimators=int(row['n_estimators']), learning_rate=row['learning_rate'],\n",
        "                                max_depth=int(row['max_depth']), random_state=int(row['random_state']))\n",
        "    xgboost_clf.fit(X_train, y_train)\n",
        "    \n",
        "    y_true = y_val\n",
        "    y_pred = xgboost_clf.predict(X_val)\n",
        "    evaluation_results = evaluation_metrics(y_true, y_pred)\n",
        "\n",
        "    models = models.append({'model_name': model_name, \n",
        "                            'model': xgboost_clf, \n",
        "                            'parameters': xgboost_clf.get_params()}, \n",
        "                           ignore_index=True)\n",
        "    \n",
        "    models_eval = models_eval.append({'model_name': model_name, \n",
        "                                      'confusion_matrix' : evaluation_results[0], \n",
        "                                      'accuracy': evaluation_results[1], \n",
        "                                      'recall' : evaluation_results[2], \n",
        "                                      'f1_score': evaluation_results[3], \n",
        "                                      'roc_auc_score': evaluation_results[4]}, \n",
        "                                     ignore_index=True)"
      ],
      "metadata": {
        "id": "evYZbB-MZVmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LightGBM\n",
        "\n",
        "- Library: lightbgm"
      ],
      "metadata": {
        "id": "p3iAdzksueOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "name = 'lightgbm_clf'\n",
        "\n",
        "train = pd.DataFrame(columns = ['n_estimators', 'learning_rate', 'max_depth', 'random_state'])\n",
        "train = train.append({'n_estimators': 1000, 'learning_rate':0.1, 'max_depth':3, 'random_state':10}, ignore_index=True)\n",
        "train = train.append({'n_estimators': 500, 'learning_rate':0.5, 'max_depth':1, 'random_state':10}, ignore_index=True)\n",
        "train = train.append({'n_estimators': 750, 'learning_rate':0.01, 'max_depth':10, 'random_state':10}, ignore_index=True)\n",
        "\n",
        "train = train.reset_index()\n",
        "for index, row in train.iterrows():\n",
        "    model_name = name + str(index)\n",
        "    lightgbm_clf = LGBMClassifier(n_estimators=int(row['n_estimators']), learning_rate=row['learning_rate'],\n",
        "                                max_depth=int(row['max_depth']), random_state=int(row['random_state']))\n",
        "    lightgbm_clf.fit(X_train, y_train)\n",
        "    \n",
        "    y_true = y_val\n",
        "    y_pred = lightgbm_clf.predict(X_val)\n",
        "    evaluation_results = evaluation_metrics(y_true, y_pred)   \n",
        "\n",
        "    models = models.append({'model_name': model_name, \n",
        "                            'model': lightgbm_clf, \n",
        "                            'parameters': lightgbm_clf.get_params()}, \n",
        "                           ignore_index=True)\n",
        "    \n",
        "    models_eval = models_eval.append({'model_name': model_name, \n",
        "                                      'confusion_matrix' : evaluation_results[0], \n",
        "                                      'accuracy': evaluation_results[1], \n",
        "                                      'recall' : evaluation_results[2], \n",
        "                                      'f1_score': evaluation_results[3], \n",
        "                                      'roc_auc_score': evaluation_results[4]}, \n",
        "                                     ignore_index=True)"
      ],
      "metadata": {
        "id": "sh_I5t-TZVaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Artificial Neural Network\n",
        "\n",
        "- Library: Keras, Tensorflow"
      ],
      "metadata": {
        "id": "QpthCMRHuhWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(10)\n",
        "\n",
        "print(tf.__version__)\n",
        "print(keras.__version__)"
      ],
      "metadata": {
        "id": "fJ6kS48GZVH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experiment 1: Base Model"
      ],
      "metadata": {
        "id": "vivojhGnfeEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "metadata": {
        "id": "XO7_fl_pfaEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann_clf = keras.models.Sequential([\n",
        "    keras.layers.Dense(17, input_shape=(X_train.shape[1],), activation='relu'), # 16 columns. 1 bias term to accelerate activation of a node.\n",
        "    keras.layers.Dense(8, activation='relu'), # One hidden layer is sufficient for the large majority of problems. \n",
        "    # Set the number of neurons in the hidden layer as the mean of the neurons in the input and output layers.\n",
        "    keras.layers.Dense(1, activation='sigmoid'), # Only 1 acceptable unless softmax activation function is used\n",
        "])\n",
        "\n",
        "ann_clf.summary()"
      ],
      "metadata": {
        "id": "OVpParvzfhtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann_clf.layers"
      ],
      "metadata": {
        "id": "eLJhQfiHfi14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle Data since SMOTE appends many 1s at the end\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "X_train_ANN, y_train_ANN = shuffle(X_train, y_train, random_state = 10)\n",
        "y_train_ANN.value_counts()"
      ],
      "metadata": {
        "id": "ldBEbhztfjvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann_clf.compile(optimizer = 'adam', \n",
        "                metrics=['accuracy'], \n",
        "                loss ='binary_crossentropy')\n",
        "\n",
        "record = ann_clf.fit(\n",
        "            X_train_ANN, \n",
        "            y_train_ANN, \n",
        "            validation_data = (X_val, y_val), \n",
        "            batch_size = 10, \n",
        "            epochs = 50)"
      ],
      "metadata": {
        "id": "bK1SXteMfln5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(ann_clf, show_shapes=True, rankdir=\"LR\")"
      ],
      "metadata": {
        "id": "ChuhGr3Ufmgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, train_acc = ann_clf.evaluate(X_train, y_train, verbose=0)\n",
        "_, val_acc = ann_clf.evaluate(X_val, y_val, verbose=0)\n",
        "_, test_acc = ann_clf.evaluate(X_test, y_test, verbose=0)\n",
        "print('Train: %.3f, Validation: %.3f, Test: %.3f' % (train_acc, val_acc, test_acc))"
      ],
      "metadata": {
        "id": "rsSZTcIlfnQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(record.history['accuracy'], label='Training')\n",
        "plt.plot(record.history['val_accuracy'], label='Validation')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs', fontsize=16)\n",
        "plt.ylabel('Accuracy', fontsize=16)\n",
        "plt.title('Accuracy Curves', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jvmnf03BfoDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot training history\n",
        "plt.plot(record.history['loss'], label='Training')\n",
        "plt.plot(record.history['val_loss'], label='Validation')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs', fontsize=14)\n",
        "plt.ylabel('Loss', fontsize=14)\n",
        "plt.title('Loss Curves', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bk9KBNCCfpw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experiment 2: Different Batch Sizes"
      ],
      "metadata": {
        "id": "hy1WHdJSfrl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit a model and plot learning curve\n",
        "def fit_model_1(X_train, y_train, X_test, y_test, n_batch):\n",
        "  # Define Model\n",
        "  ann_clf = keras.models.Sequential([\n",
        "      keras.layers.Dense(6, input_shape=(X_train.shape[1],), activation='sigmoid'),\n",
        "      keras.layers.Dense(6, activation='sigmoid'),\n",
        "      keras.layers.Dense(6, activation='sigmoid'), \n",
        "      # keras.layers.Dropout(0.2),\n",
        "      keras.layers.Dense(1, activation = 'sigmoid')\n",
        "  ])\n",
        "\n",
        "  # Compile Model\n",
        "  ann_clf.compile(optimizer = 'adam',\n",
        "                metrics=['accuracy'],\n",
        "                loss = 'binary_crossentropy')\n",
        "  \n",
        "  # Fit Model\n",
        "  history = ann_clf.fit(X_train_ANN,\n",
        "                      y_train_ANN,\n",
        "                      validation_data=(X_test, y_test),\n",
        "                      epochs=100,\n",
        "                      verbose=0,\n",
        "                      batch_size=n_batch)\n",
        "\n",
        "  # Plot Learning Curves\n",
        "  plt.plot(history.history['accuracy'], label='train') \n",
        "  plt.plot(history.history['val_accuracy'], label='test') \n",
        "  plt.title('batch='+str(n_batch)) \n",
        "  plt.legend()"
      ],
      "metadata": {
        "id": "EbpRvs_nfsV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create learning curves for different batch sizes\n",
        "# batch_sizes = [4, 6, 10, 16, 32, 64, 128, 260]\n",
        "batch_sizes = [10, 15, 20, 25, 30]\n",
        "\n",
        "plt.figure(figsize=(10,15))\n",
        "for i in range(len(batch_sizes)):\n",
        "\n",
        "  # Determine the Plot Number\n",
        "  plot_no = 420 + (i+1)\n",
        "  plt.subplot(plot_no)\n",
        "\n",
        "  # Fit model and plot learning curves for a batch size\n",
        "  fit_model_1(X_train, y_train, X_test, y_test, batch_sizes[i])\n",
        "\n",
        "# Show learning curves\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "govzlyT3fwI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experiment 3: Different EPOCHs"
      ],
      "metadata": {
        "id": "jDdPwZjcfxsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit a model and plot learning curve\n",
        "def fit_model_2(trainX, trainy, validX, validy, n_epoch):\n",
        "  # Define Model\n",
        "  ann_clf = keras.models.Sequential([\n",
        "      keras.layers.Dense(6, input_shape=(X_train.shape[1],), activation='sigmoid'),\n",
        "      keras.layers.Dense(6, activation='sigmoid'),\n",
        "      keras.layers.Dense(6, activation='sigmoid'), \n",
        "      # keras.layers.Dropout(0.2),\n",
        "      keras.layers.Dense(1, activation = 'sigmoid')\n",
        "  ])\n",
        "\n",
        "  # Compile Model\n",
        "  ann_clf.compile(optimizer = 'adam',\n",
        "                metrics=['accuracy'],\n",
        "                loss = 'binary_crossentropy')\n",
        "    \n",
        "  # fit model\n",
        "  history = ann_clf.fit(X_train_ANN,\n",
        "                      y_train_ANN,\n",
        "                      validation_data=(X_test, y_test),\n",
        "                      epochs=n_epoch,\n",
        "                      verbose=0,\n",
        "                      batch_size=6)\n",
        "    \n",
        "  # plot learning curves\n",
        "  plt.plot(history.history['accuracy'], label='train')\n",
        "  plt.plot(history.history['val_accuracy'], label='test')\n",
        "  plt.title('epoch='+str(n_epoch))\n",
        "  plt.legend()"
      ],
      "metadata": {
        "id": "gjSGc9u6fxTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create learning curves for different batch sizes\n",
        "# epochs = [20, 50, 100, 120, 150, 200, 300, 400]\n",
        "epochs = [50, 60, 70, 80, 90, 100]\n",
        "\n",
        "plt.figure(figsize=(10,15))\n",
        "for i in range(len(batch_sizes)):\n",
        "\n",
        "  # Determine the Plot Number\n",
        "  plot_no = 420 + (i+1)\n",
        "  plt.subplot(plot_no)\n",
        "\n",
        "  # Fit model and plot learning curves for a batch size\n",
        "  fit_model_2(X_train, y_train, X_test, y_test, epochs[i])\n",
        "\n",
        "# Show learning curves\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SP2zm_ySfznt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experiment 4: Early Stopping"
      ],
      "metadata": {
        "id": "3_QvFWOmf0JZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "3AUOp_Qwf2_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_model():\n",
        "  # Define Model\n",
        "  ann_clf = keras.models.Sequential([\n",
        "      keras.layers.Dense(6, input_shape=(X_train.shape[1],), activation='sigmoid'),\n",
        "      keras.layers.Dense(6, activation='sigmoid'),\n",
        "      keras.layers.Dense(6, activation='sigmoid'), \n",
        "      keras.layers.Dropout(0.2),\n",
        "      keras.layers.Dense(1, activation = 'sigmoid')\n",
        "  ])\n",
        "\n",
        "  # Compile Model\n",
        "  ann_clf.compile(optimizer = 'adam',\n",
        "                metrics=['accuracy'],\n",
        "                loss = 'binary_crossentropy')\n",
        "  return ann_clf\n",
        "\n",
        "# init model\n",
        "ann_clf = init_model()\n",
        "# simple early stopping\n",
        "es = EarlyStopping(monitor='val_loss',\n",
        "                   mode='min',\n",
        "                   verbose=1,\n",
        "                   patience=150)\n",
        "mc = ModelCheckpoint('best_model.h5',\n",
        "                     monitor='val_accuracy',\n",
        "                     mode='max',\n",
        "                     verbose=1,\n",
        "                     save_best_only=True)\n",
        "history = ann_clf.fit(X_train_ANN,\n",
        "                    y_train_ANN,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    epochs=50,\n",
        "                    verbose=0,\n",
        "                    batch_size=25,\n",
        "                    callbacks=[es, mc])"
      ],
      "metadata": {
        "id": "8uidzovhf4S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot training history\n",
        "plt.plot(history.history['loss'], label='Training')\n",
        "plt.plot(history.history['val_loss'], label='Validation')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs', fontsize=14)\n",
        "plt.ylabel('Loss', fontsize=14)\n",
        "plt.title('Loss Curves', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JAVLJDzLf6Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=[8,5])\n",
        "plt.plot(history.history['accuracy'], label='Training')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs', fontsize=16)\n",
        "plt.ylabel('Accuracy', fontsize=16)\n",
        "plt.title('Accuracy Curves', fontsize=16)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sPPqcZCyf6zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble Learning\n",
        "\n",
        "- Library: Scikit-learn, Keras, Tensorflow\n",
        "- Shuffling does not affect the model building. No random_seed.\n"
      ],
      "metadata": {
        "id": "4rBdGe3Gukv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(models_eval)"
      ],
      "metadata": {
        "id": "YCHV8rR-ZU5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-Train Top 3 Models Using Their Parameters Specifically\n",
        "\n",
        "model_1 = LGBMClassifier(n_estimators=1000, learning_rate=0.1, max_depth=3, random_state=10)\n",
        "model_2 = GradientBoostingClassifier(n_estimators=500, learning_rate=0.5, max_depth=1, random_state=10)\n",
        "model_3 = RandomForestClassifier(n_estimators=1000, max_depth = None, n_jobs =-1, random_state=10)\n",
        "\n",
        "name = 'ensem_clf'\n",
        "\n",
        "train = pd.DataFrame(columns = ['voting', 'n_jobs'])\n",
        "train = train.append({'voting': 'hard', 'n_jobs': -1}, ignore_index=True)\n",
        "train = train.append({'voting': 'soft', 'n_jobs': -1}, ignore_index=True)\n",
        "train = train.reset_index()\n",
        "\n",
        "for index, row in train.iterrows():\n",
        "    model_name = name + str(index)\n",
        "    ens_clf = VotingClassifier(estimators=[('m1', model_1), ('m2', model_2), ('m3', model_3)],\n",
        "                               voting = row['voting'],\n",
        "                               n_jobs = int(row['n_jobs']))\n",
        "    ens_clf.fit(X_train, y_train)\n",
        "    y_true = y_val\n",
        "    y_pred = ens_clf.predict(X_val)\n",
        "    evaluation_results = evaluation_metrics(y_true, y_pred)\n",
        "\n",
        "    models = models.append({'model_name': model_name, \n",
        "                            'model': ens_clf, \n",
        "                            'parameters': ens_clf.get_params()}, \n",
        "                           ignore_index=True)\n",
        "    \n",
        "    models_eval = models_eval.append({'model_name': model_name, \n",
        "                                      'confusion_matrix' : evaluation_results[0], \n",
        "                                      'accuracy': evaluation_results[1], \n",
        "                                      'recall' : evaluation_results[2], \n",
        "                                      'f1_score': evaluation_results[3], \n",
        "                                      'roc_auc_score': evaluation_results[4]}, \n",
        "                                     ignore_index=True)"
      ],
      "metadata": {
        "id": "YIe8XVe4gBMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion"
      ],
      "metadata": {
        "id": "VSpOrrm2gNXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(models)"
      ],
      "metadata": {
        "id": "EZfBQi4KgLrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(models_eval)"
      ],
      "metadata": {
        "id": "2LY9af70gM_6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}